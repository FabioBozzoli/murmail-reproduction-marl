"""
true_exploitability.py - Correct Nash exploitability for correlated joint policies

This implements the THEORETICALLY CORRECT exploitability calculation
using occupancy measure formulation and linear programming.

For a correlated joint policy π(a_s, a_l | s), true exploitability
accounts for correlation breakage when agents deviate.

Key differences from approximation:
1. Uses occupancy measures μ(s, a_s, a_l) 
2. Solves LP for best response
3. Correctly handles correlation breakage
4. No marginal policy assumptions

Reference: "Computing Approximate Equilibria in Sequential Adversarial Games"
"""

import numpy as np
from scipy.optimize import linprog
from scipy.sparse import csr_matrix

def calc_true_exploitability(
    pi_joint,           # (num_states, num_actions_speaker * num_actions_listener)
    R,                  # (num_states, num_actions_speaker, num_actions_listener)
    P,                  # (num_states, num_actions_speaker, num_actions_listener, num_states)
    init_dist,          # (num_states,)
    gamma,              # discount factor
    num_actions_speaker=3,
    num_actions_listener=5,
):
    """
    Calculate TRUE Nash exploitability for correlated joint policy using LP.
    
    Exploitability = ε_speaker + ε_listener where each ε is computed by
    solving a linear program over occupancy measures.
    
    This correctly accounts for:
    - Correlation in joint policy
    - Correlation breakage on deviation
    - Best response conditioning
    
    Args:
        pi_joint: Joint policy (S, A_s × A_l)
        R: Rewards (S, A_s, A_l)
        P: Transitions (S, A_s, A_l, S')
        init_dist: Initial state distribution (S,)
        gamma: Discount factor
        
    Returns:
        exploitability: True Nash gap
    """
    
    num_states = pi_joint.shape[0]
    num_joint_actions = num_actions_speaker * num_actions_listener
    
    print(f"Computing true exploitability...")
    print(f"  States: {num_states}")
    print(f"  Joint actions: {num_joint_actions}")
    
    # Step 1: Compute value of joint policy
    V_joint, mu_joint = compute_joint_policy_value(
        pi_joint, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    print(f"  Joint policy value: {V_joint:.6f}")
    
    # Step 2: Best response for speaker (LP)
    V_br_speaker = best_response_speaker_lp(
        pi_joint, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    epsilon_speaker = V_br_speaker - V_joint
    print(f"  Speaker BR value: {V_br_speaker:.6f}, ε_speaker: {epsilon_speaker:.6f}")
    
    # Step 3: Best response for listener (LP)
    V_br_listener = best_response_listener_lp(
        pi_joint, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    epsilon_listener = V_br_listener - V_joint
    print(f"  Listener BR value: {V_br_listener:.6f}, ε_listener: {epsilon_listener:.6f}")
    
    # Total exploitability
    exploitability = epsilon_speaker + epsilon_listener
    print(f"  Total exploitability: {exploitability:.6f}")
    
    return exploitability


def decode_joint_action(a_joint, num_actions_listener):
    """Decode joint action to (a_speaker, a_listener)"""
    a_speaker = a_joint // num_actions_listener
    a_listener = a_joint % num_actions_listener
    return a_speaker, a_listener


def encode_joint_action(a_speaker, a_listener, num_actions_listener):
    """Encode (a_speaker, a_listener) to joint action"""
    return a_speaker * num_actions_listener + a_listener


def compute_joint_policy_value(pi_joint, R, P, init_dist, gamma,
                                num_actions_speaker, num_actions_listener):
    """
    Compute value and occupancy measure of joint policy.
    
    Occupancy measure: μ(s, a) = (1-γ) Σ_t γ^t P(s_t=s, a_t=a | π)
    
    Returns:
        V: Expected discounted value
        mu: Occupancy measure (num_states, num_joint_actions)
    """
    num_states = pi_joint.shape[0]
    num_joint_actions = num_actions_speaker * num_actions_listener
    
    # Compute state occupancy via power iteration
    # d_π(s) = (1-γ) Σ_t γ^t P(s_t=s | π)
    
    # Build transition matrix under policy
    # T[s', s] = Σ_a π(a|s) P(s'|s,a)
    T = np.zeros((num_states, num_states))
    
    for s in range(num_states):
        for a_joint in range(num_joint_actions):
            prob = pi_joint[s, a_joint]
            if prob > 0:
                a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
                T[:, s] += prob * P[s, a_s, a_l, :]
    
    # Solve for state occupancy: d = (1-γ)d₀ + γT^T d
    # d = (I - γT^T)^{-1} (1-γ)d₀
    I = np.eye(num_states)
    d = np.linalg.solve(I - gamma * T.T, (1 - gamma) * init_dist)
    
    # Compute occupancy measure μ(s,a) = d(s) π(a|s)
    mu = np.zeros((num_states, num_joint_actions))
    for s in range(num_states):
        mu[s, :] = d[s] * pi_joint[s, :]
    
    # Compute value
    V = 0.0
    for s in range(num_states):
        for a_joint in range(num_joint_actions):
            if mu[s, a_joint] > 0:
                a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
                V += mu[s, a_joint] * R[s, a_s, a_l] / (1 - gamma)
    
    return V, mu


def best_response_speaker_lp(pi_joint, R, P, init_dist, gamma,
                             num_actions_speaker, num_actions_listener):
    """
    Compute speaker's best response value via LP over occupancy measures.
    
    Speaker chooses occupancy μ_s(s, a_s, a_l) to maximize expected reward,
    given that listener plays according to conditional distribution from π_joint.
    
    LP formulation:
    max Σ_{s,a_s,a_l} μ(s,a_s,a_l) R(s,a_s,a_l)
    
    s.t.
    - Occupancy flow constraints
    - Listener plays conditionally: μ(s,a_s,a_l) ∝ π_joint(a_s,a_l|s)
    - Non-negativity
    """
    num_states = pi_joint.shape[0]
    num_joint_actions = num_actions_speaker * num_actions_listener
    
    # Decision variables: μ(s, a_s, a_l) for all s, a_s, a_l
    # Flatten to vector: [μ(s=0,a=0), μ(s=0,a=1), ..., μ(s=S-1,a=A-1)]
    num_vars = num_states * num_joint_actions
    
    # Objective: maximize Σ μ(s,a) R(s,a)
    # For linprog (minimization), negate coefficients
    c = np.zeros(num_vars)
    
    for s in range(num_states):
        for a_joint in range(num_joint_actions):
            a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
            idx = s * num_joint_actions + a_joint
            c[idx] = -R[s, a_s, a_l]  # Negative for maximization
    
    # Constraints
    A_eq_list = []
    b_eq_list = []
    A_ub_list = []
    b_ub_list = []
    
    # 1. Flow constraints: for each state s'
    #    Σ_a μ(s',a) = (1-γ)ρ₀(s') + γ Σ_{s,a} μ(s,a) P(s'|s,a)
    
    for s_next in range(num_states):
        constraint = np.zeros(num_vars)
        
        # LHS: Σ_a μ(s',a)
        for a_joint in range(num_joint_actions):
            idx = s_next * num_joint_actions + a_joint
            constraint[idx] += 1.0
        
        # RHS incoming flow: γ Σ_{s,a} μ(s,a) P(s'|s,a)
        for s in range(num_states):
            for a_joint in range(num_joint_actions):
                a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
                idx = s * num_joint_actions + a_joint
                constraint[idx] -= gamma * P[s, a_s, a_l, s_next]
        
        A_eq_list.append(constraint)
        b_eq_list.append((1 - gamma) * init_dist[s_next])
    
    # 2. Listener conditioning constraints
    # For each (s, a_s), the distribution over a_l must match π_joint conditionally
    #
    # If speaker chooses a_s in state s with total mass m_s,a_s,
    # then: μ(s,a_s,a_l) = m_s,a_s × π_joint(a_l | s, a_s)
    #
    # where π_joint(a_l | s, a_s) = π_joint(s, a_s, a_l) / Σ_a_l' π_joint(s, a_s, a_l')
    
    for s in range(num_states):
        for a_s in range(num_actions_speaker):
            # Compute marginal π_speaker(a_s | s) from joint
            marginal_as = 0.0
            for a_l in range(num_actions_listener):
                a_joint = encode_joint_action(a_s, a_l, num_actions_listener)
                marginal_as += pi_joint[s, a_joint]
            
            if marginal_as < 1e-10:
                # Speaker never plays a_s in state s, skip
                continue
            
            # For each pair (a_l, a_l'), enforce:
            # μ(s,a_s,a_l) / π(a_s,a_l|s) = μ(s,a_s,a_l') / π(a_s,a_l'|s)
            #
            # Rearrange: μ(s,a_s,a_l) × π(a_s,a_l'|s) = μ(s,a_s,a_l') × π(a_s,a_l|s)
            
            for a_l1 in range(num_actions_listener):
                for a_l2 in range(a_l1 + 1, num_actions_listener):
                    a_joint1 = encode_joint_action(a_s, a_l1, num_actions_listener)
                    a_joint2 = encode_joint_action(a_s, a_l2, num_actions_listener)
                    
                    prob1 = pi_joint[s, a_joint1]
                    prob2 = pi_joint[s, a_joint2]
                    
                    if prob1 < 1e-10 and prob2 < 1e-10:
                        continue
                    
                    constraint = np.zeros(num_vars)
                    idx1 = s * num_joint_actions + a_joint1
                    idx2 = s * num_joint_actions + a_joint2
                    
                    # μ₁ × prob₂ - μ₂ × prob₁ = 0
                    constraint[idx1] = prob2
                    constraint[idx2] = -prob1
                    
                    A_eq_list.append(constraint)
                    b_eq_list.append(0.0)
    
    # Convert to arrays
    A_eq = np.array(A_eq_list) if A_eq_list else None
    b_eq = np.array(b_eq_list) if b_eq_list else None
    
    # Bounds: μ >= 0
    bounds = [(0, None) for _ in range(num_vars)]
    
    # Solve LP
    result = linprog(
        c, A_eq=A_eq, b_eq=b_eq,
        bounds=bounds,
        method='highs',
        options={'disp': False, 'presolve': True}
    )
    
    if not result.success:
        print(f"  Warning: Speaker LP did not converge: {result.message}")
        return -np.inf
    
    # Return negative of objective (we minimized -value)
    return -result.fun / (1 - gamma)


def best_response_listener_lp(pi_joint, R, P, init_dist, gamma,
                              num_actions_speaker, num_actions_listener):
    """
    Compute listener's best response value via LP.
    
    Symmetric to speaker BR but listener optimizes over a_l conditioned on speaker.
    """
    num_states = pi_joint.shape[0]
    num_joint_actions = num_actions_speaker * num_actions_listener
    
    num_vars = num_states * num_joint_actions
    
    # Objective
    c = np.zeros(num_vars)
    for s in range(num_states):
        for a_joint in range(num_joint_actions):
            a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
            idx = s * num_joint_actions + a_joint
            c[idx] = -R[s, a_s, a_l]
    
    # Constraints
    A_eq_list = []
    b_eq_list = []
    
    # Flow constraints
    for s_next in range(num_states):
        constraint = np.zeros(num_vars)
        
        for a_joint in range(num_joint_actions):
            idx = s_next * num_joint_actions + a_joint
            constraint[idx] += 1.0
        
        for s in range(num_states):
            for a_joint in range(num_joint_actions):
                a_s, a_l = decode_joint_action(a_joint, num_actions_listener)
                idx = s * num_joint_actions + a_joint
                constraint[idx] -= gamma * P[s, a_s, a_l, s_next]
        
        A_eq_list.append(constraint)
        b_eq_list.append((1 - gamma) * init_dist[s_next])
    
    # Speaker conditioning constraints
    for s in range(num_states):
        for a_l in range(num_actions_listener):
            # Marginal π_listener(a_l | s)
            marginal_al = 0.0
            for a_s in range(num_actions_speaker):
                a_joint = encode_joint_action(a_s, a_l, num_actions_listener)
                marginal_al += pi_joint[s, a_joint]
            
            if marginal_al < 1e-10:
                continue
            
            # Enforce ratio constraints
            for a_s1 in range(num_actions_speaker):
                for a_s2 in range(a_s1 + 1, num_actions_speaker):
                    a_joint1 = encode_joint_action(a_s1, a_l, num_actions_listener)
                    a_joint2 = encode_joint_action(a_s2, a_l, num_actions_listener)
                    
                    prob1 = pi_joint[s, a_joint1]
                    prob2 = pi_joint[s, a_joint2]
                    
                    if prob1 < 1e-10 and prob2 < 1e-10:
                        continue
                    
                    constraint = np.zeros(num_vars)
                    idx1 = s * num_joint_actions + a_joint1
                    idx2 = s * num_joint_actions + a_joint2
                    
                    constraint[idx1] = prob2
                    constraint[idx2] = -prob1
                    
                    A_eq_list.append(constraint)
                    b_eq_list.append(0.0)
    
    A_eq = np.array(A_eq_list) if A_eq_list else None
    b_eq = np.array(b_eq_list) if b_eq_list else None
    
    bounds = [(0, None) for _ in range(num_vars)]
    
    result = linprog(
        c, A_eq=A_eq, b_eq=b_eq,
        bounds=bounds,
        method='highs',
        options={'disp': False, 'presolve': True}
    )
    
    if not result.success:
        print(f"  Warning: Listener LP did not converge: {result.message}")
        return -np.inf
    
    return -result.fun / (1 - gamma)


# ============= WRAPPER FOR Q-NETWORK =============

def calc_true_exploitability_from_q_network(q_network, device, R, P, init_dist, gamma,
                                            num_states, num_actions_speaker, num_actions_listener):
    """
    Calculate true exploitability directly from Q-network.
    """
    import torch
    
    num_joint_actions = num_actions_speaker * num_actions_listener
    
    # Extract deterministic joint policy
    pi_joint = np.zeros((num_states, num_joint_actions))
    
    with torch.no_grad():
        for s in range(num_states):
            state_t = torch.LongTensor([s]).to(device)
            q_vals = q_network(state_t).cpu().numpy()[0]
            best_action = q_vals.argmax()
            pi_joint[s, best_action] = 1.0
    
    # Calculate true exploitability
    gap = calc_true_exploitability(
        pi_joint, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    return gap


# ============= TESTS =============

if __name__ == "__main__":
    print("="*70)
    print("Testing TRUE exploitability with LP")
    print("="*70)
    
    # Simple 2-state coordination game
    num_states = 2
    num_actions_speaker = 2
    num_actions_listener = 2
    num_joint_actions = 4
    gamma = 0.9
    
    # Coordination rewards
    R = np.zeros((num_states, num_actions_speaker, num_actions_listener))
    R[0, 0, 0] = 1.0
    R[0, 1, 1] = 1.0
    R[1, 0, 1] = 1.0
    R[1, 1, 0] = 1.0
    
    # Transitions
    P = np.zeros((num_states, num_actions_speaker, num_actions_listener, num_states))
    P[0, :, :, 1] = 1.0
    P[1, :, :, 0] = 1.0
    
    init_dist = np.array([0.5, 0.5])
    
    # Test 1: Perfect coordination (deterministic)
    print("\n" + "="*70)
    print("TEST 1: Perfect coordination policy")
    print("="*70)
    
    pi_perfect = np.zeros((num_states, num_joint_actions))
    pi_perfect[0, 0] = 1.0  # (0,0) in state 0
    pi_perfect[1, 3] = 1.0  # (1,1) in state 1
    
    gap_perfect = calc_true_exploitability(
        pi_perfect, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    print(f"\n✓ Perfect policy exploitability: {gap_perfect:.6f}")
    print("  Expected: ≈0 (already coordinated)")
    
    # Test 2: Random uniform policy
    print("\n" + "="*70)
    print("TEST 2: Random uniform policy")
    print("="*70)
    
    pi_random = np.ones((num_states, num_joint_actions)) / num_joint_actions
    
    gap_random = calc_true_exploitability(
        pi_random, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    print(f"\n✓ Random policy exploitability: {gap_random:.6f}")
    print("  Expected: >0 (suboptimal)")
    
    # Test 3: Correlated policy
    print("\n" + "="*70)
    print("TEST 3: Correlated policy (50-50 between two equilibria)")
    print("="*70)
    
    pi_correlated = np.zeros((num_states, num_joint_actions))
    pi_correlated[0, 0] = 0.5  # 50% (0,0)
    pi_correlated[0, 3] = 0.5  # 50% (1,1)
    pi_correlated[1, 1] = 0.5  # 50% (0,1)
    pi_correlated[1, 2] = 0.5  # 50% (1,0)
    
    gap_correlated = calc_true_exploitability(
        pi_correlated, R, P, init_dist, gamma,
        num_actions_speaker, num_actions_listener
    )
    
    print(f"\n✓ Correlated policy exploitability: {gap_correlated:.6f}")
    print("  Expected: Small (still achieves coordination)")
    
    print("\n" + "="*70)
    print("✅ All tests complete")
    print("="*70)